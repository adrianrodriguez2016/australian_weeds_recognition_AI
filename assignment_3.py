# -*- coding: utf-8 -*-
"""Assignment 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kAfccSEsFoCYMnvBoF33etLYlpMHp6pF

Copyright 2018 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0
"""

!pip install -q tensorflow-datasets tensorflow

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
from tensorflow import keras

from tensorflow.keras.layers import Dense,Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import tensorflow_datasets as tfds

physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(physical_devices))
tf.config.experimental.set_memory_growth(physical_devices[0], True)

# Construct a tf.data.Dataset
valid_set, train_set, test_set = tfds.load(
    'deep_weeds', 
    as_supervised = 'true',
    split=[ [f'train[{k}%:{k+10}%]' for k in range(0, 40,10)], [f'train[:{k}%]+train[{k+10}%:40%]' for k in range(0, 40, 10)], 'train[40%:50%]'], #Half dataset
    #split=[ [f'train[{k}%:{k+10}%]' for k in range(0, 80, 10)], [f'train[:{k}%]+train[{k+10}%:80%]' for k in range(0, 80, 10)], 'train[80%:]']   #Full dataset
    )

print("Train set size: ", len(train_set)) # Number of folds:  4 (80%) half of dataset -> (40%) out of total dataset
print("Train set size: ", len(train_set[0])) # Train set size K=0:  5253 
print("Train set size: ", len(train_set[1])) # Train set size K=1:  5253 
print("Train set size: ", len(train_set[2])) # Train set size K=2:  5253 
print("Train set size: ", len(train_set[3])) # Train set size K=3:  5253 
 
print("Valid set size: ", len(valid_set)) # Number of folds:  4 (10%) half of dataset -> (5%)  out of total dataset
print("Train set size: ", len(valid_set[0])) # Train set size K=0:  1751 (5%)
print("Train set size: ", len(valid_set[1])) # Train set size K=1:  1751 (5%)
print("Train set size: ", len(valid_set[2])) # Train set size K=2: 1751 (5%)
print("Train set size: ", len(valid_set[3])) # Train set size K=3:  1751 (5%)


print("Test set size: ", len(test_set))   # Test set size:  1750 (10%) half of dataset -> (5%)  out of total dataset


print([f'train[{k}%:{k+7}%]' for k in range(0, 35, 7)]) # 
print([f'train[:{k}%]+train[{k+7}%:35%]' for k in range(0, 35, 7)])

plt.figure(figsize=(10, 10))


for i, (image, label) in enumerate(train_set[1].take(49)):
    ax = plt.subplot(7, 7, i + 1)
    plt.imshow(image)
    plt.title(int(label))
    plt.axis("off")

image_size = (256, 256)

print(train_set[0])

for i in range(len(train_set)):
  train_set[i] = train_set[i].map(lambda x, y: (tf.image.resize(x, image_size), y))

print(train_set[0])

print(valid_set[0])
for z in range(len(valid_set)):
  valid_set[z] = valid_set[z].map(lambda x, y: (tf.image.resize(x, image_size), y))

print(valid_set[0])

print(test_set)

test_set = test_set.map(lambda x, y: (tf.image.resize(x, image_size), y))

print(test_set)

batch_size = 32



print(train_set[0])
for i in range(len(train_set)):
  train_set[i] = train_set[i].cache().batch(batch_size).prefetch(buffer_size=10)
print(train_set[0])

print(valid_set[0])
for z in range(len(valid_set)):
  valid_set[z] = valid_set[z].cache().batch(batch_size).prefetch(buffer_size=10)
print(valid_set[0])

print(test_set)
test_set = test_set.cache().batch(batch_size).prefetch(buffer_size=10)
print(test_set)

from tensorflow.keras import layers


data_preprocessing = keras.Sequential(
    [
        layers.Resizing(224, 224),
        #layers.Rescaling(1./255)
    ]
)

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal_and_vertical"),
        layers.RandomRotation(1),
        layers.RandomContrast(factor=0.3),
        layers.RandomBrightness(factor=0.15),
        layers.RandomZoom(0.1)
    
    ]
)

import numpy as np



for images, labels in train_set[1].take(1):
    plt.figure(figsize=(10, 10))
    first_image = images[0]

    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0), training=True)
        plt.imshow(augmented_image[0].numpy().astype("int64"))
        plt.title(int(labels[0]))
        plt.axis("off")

pretrained_model= tf.keras.applications.ResNet50(
      
    include_top=False,
    input_shape=(224,224,3),
    pooling='avg',classes=9,
    weights='imagenet'

    )


pretrained_model.trainable=False

kfolds = len(train_set) 

accuracy_per_fold = []
loss_per_fold = []

for fold_no in range(kfolds):


  resnet_model = keras.Sequential([

      data_preprocessing,
      data_augmentation,
      pretrained_model,
      layers.Flatten(),
      layers.Dense(512,activation='relu'),
      layers.Dropout(0.5),
      layers.Dense(9, activation='softmax')
  ])

  resnet_model.compile(optimizer=Adam(),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

  epochs = 20

  
  
  # Generate a print
  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  # Fit data to model
  history = resnet_model.fit(
      train_set[fold_no],
      batch_size=batch_size,
      epochs=epochs,
      verbose=1,
      validation_data=valid_set[fold_no], 
      )

  # Generate generalization metrics
  scores = resnet_model.evaluate(valid_set[fold_no], verbose=0)
  print(f'Score for fold {fold_no}: {resnet_model.metrics_names[0]} of {scores[0]}; {resnet_model.metrics_names[1]} of {scores[1]*100}%')
  accuracy_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('Model accuracy  Fold number:' + str(fold_no+1))
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'validation'], loc='upper left')
  plt.show()

  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('Model loss Fold number:' + str(fold_no+1))
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'validation'], loc='upper left')
  plt.show()

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(accuracy_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {accuracy_per_fold[i]}%')
print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(accuracy_per_fold)} (+- {np.std(accuracy_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')


best_fold = int(accuracy_per_fold.index(max(accuracy_per_fold)))

print("> Fold with best accuracy: " + str(1 + best_fold))

resnet_model.summary()

pretrained_model.trainable=True

pretrained_model.summary()

resnet_model.compile(optimizer=Adam(lr=1e-5),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

resnet_model.summary()

epochs = 50

# Fit data to model
history2 = resnet_model.fit(
    train_set[best_fold],
    batch_size=batch_size,
    epochs=epochs,
    verbose=1,
    validation_data=valid_set[best_fold], 
  )

  
plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('(Fine Tuning) Model accuracy (Best fold: ' + str(best_fold) + ")")
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('(Fine Tuning) Model loss (Best fold: ' + str(best_fold) + ")")
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.title('(Fine Tuning) Model accuracy (Best fold: ' + str(best_fold +1) + ")")
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('(Fine Tuning) Model loss (Best fold: ' + str(best_fold+1) + ")")
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.show()

predictions = resnet_model.predict(test_set)
predictions=np.argmax(predictions,axis=1)

predictions

# Evaluate the best model with testing data.
print(resnet_model.evaluate(x=test_set))

test_labels=np.array([])
for i in test_set:
  test_labels=np.append(test_labels, i[1].numpy())

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(test_labels, predictions))
print("Precision:",metrics.precision_score(test_labels, predictions, average = 'weighted'))
print("Recall:",metrics.recall_score(test_labels, predictions, average = 'weighted'))
print("F1-score:",metrics.f1_score(test_labels, predictions, average = 'weighted'))

index = 0
misclassifiedIndexes = []
for label, predict in zip(test_labels, predictions):
  if label != predict:
    misclassifiedIndexes.append(index)
  index +=1

print(len(test_labels))
print(len(misclassifiedIndexes))

class_names = ["Chinee apple", "Lantana", "Parkinsonia", "Parthenium", "Prickly acacia", "Rubber vine", "Siam weed", "Snake weed", "Negative"]

for images, labels in test_set.take(1):
  i = 0
  plt.figure(figsize=(10, 10))
  
  for badIndex in misclassifiedIndexes[0:3]:
    ax = plt.subplot(1, 3, i + 1)
    image_to_show = images[badIndex]
    plt.imshow(image_to_show.numpy().astype("int64"))
    plt.title('Predicted: {}, True: {}'.format(class_names[predictions[badIndex].astype(np.int64)], class_names[test_labels[badIndex].astype(np.int64)]), fontsize = 10)
    plt.axis("off")

    i += 1

!pip install pyyaml h5py  # Required to save models in HDF5 format

resnet_model.save('DeepWeedsResNet50FineTuning.h5')